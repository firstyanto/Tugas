{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa2ef10",
   "metadata": {},
   "source": [
    "# Mendeteksi Anomali pada Jaringan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4b094",
   "metadata": {},
   "source": [
    "Anomaly detection adalah teknik pelatihan unsupervised yang menganalisis sejauh mana data yang masuk berbeda dari data yang digunakan untuk melatih jaringan saraf. Secara tradisional, para ahli keamanan siber menggunakan anomaly detection untuk memastikan keamanan jaringan. Namun, dalam ilmu data (data science), teknik ini juga dapat digunakan untuk mendeteksi masukan (input) yang tidak pernah digunakan dalam proses pelatihan jaringan saraf.\n",
    "\n",
    "Terdapat beberapa dataset yang umum digunakan untuk mendemonstrasikan anomaly detection. Pada kajian kali ini, kita akan membahas dataset KDD-99.\n",
    "- [Stratosphere IPS Dataset](https://www.stratosphereips.org/category/dataset.html)\n",
    "- [The ADFA Intrusion Detection Datasets (2013) - for HIDS](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-IDS-Datasets/)\n",
    "- [ITOC CDX (2009)](https://westpoint.edu/centers-and-research/cyber-research-center/data-sets)\n",
    "- [KDD-99 Dataset](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981637c9",
   "metadata": {},
   "source": [
    "## Dataset KDD-99\n",
    "\n",
    "Meskipun dataset KDD99 sudah berusia lebih dari 20 tahun, dataset ini masih banyak digunakan untuk mendemonstrasikan Intrusion Detection Systems (IDS) dan Anomaly Detection. KDD99 merupakan dataset yang digunakan dalam The Third International Knowledge Discovery and Data Mining Tools Competition, yang diselenggarakan bersamaan dengan KDD-99, The Fifth International Conference on Knowledge Discovery and Data Mining.\n",
    "Tugas dalam kompetisi tersebut adalah membangun sebuah pendeteksi intrusi jaringan, yaitu model prediktif yang mampu membedakan antara koneksi “buruk” (disebut intrusi atau serangan) dan koneksi “baik” (normal). Basis data ini berisi sekumpulan data standar untuk diaudit, termasuk berbagai jenis intrusi yang disimulasikan dalam lingkungan jaringan militer.\n",
    "\n",
    "Kode berikut digunakan untuk membaca dataset KDD99 dalam format CSV ke dalam data frame Pandas. Format standar KDD99 tidak menyertakan nama kolom, sehingga program menambahkannya secara manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9433c9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kdd-with-columns.csv\n",
      "Read 494021 rows.\n",
      "        duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
      "0              0           tcp  ...                      0.0  normal.\n",
      "1              0           tcp  ...                      0.0  normal.\n",
      "...          ...           ...  ...                      ...      ...\n",
      "494019         0           tcp  ...                      0.0  normal.\n",
      "494020         0           tcp  ...                      0.0  normal.\n",
      "\n",
      "[494021 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Set Pandas display options\n",
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "# Download the file using urllib\n",
    "url = 'https://github.com/jeffheaton/jheaton-ds2/raw/main/kdd-with-columns.csv'\n",
    "filename = 'kdd-with-columns.csv'\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    except:\n",
    "        print('Error downloading')\n",
    "        raise\n",
    "\n",
    "print(filename)\n",
    "\n",
    "# Original file: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(\"Read {} rows.\".format(len(df)))\n",
    "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
    "df.dropna(inplace=True, axis=1) \n",
    "# For now, just drop NA's (rows with missing values)\n",
    "\n",
    "# Display 5 rows\n",
    "pd.set_option('display.max_columns', 5)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a87747",
   "metadata": {},
   "source": [
    "Dataset KDD99 berisi banyak kolom yang menggambarkan kondisi jaringan selama interval waktu tertentu, di mana serangan siber mungkin terjadi. Kolom “outcome” menunjukkan apakah koneksi tersebut “normal” (tidak ada serangan) atau jenis serangan yang dilakukan. Kode berikut menampilkan jumlah kemunculan untuk setiap jenis serangan dan data “normal”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3d98169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "back.               2203\n",
       "buffer_overflow.      30\n",
       "                    ... \n",
       "warezclient.        1020\n",
       "warezmaster.          20\n",
       "Name: outcome, Length: 23, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('outcome')['outcome'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6196096",
   "metadata": {},
   "source": [
    "## Pemrosesan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb583261",
   "metadata": {},
   "source": [
    "Kita perlu melakukan beberapa tahap prapemrosesan sebelum data KDD99 dapat dimasukkan ke dalam jaringan saraf (neural network). Dua fungsi berikut disediakan untuk membantu proses prapemrosesan tersebut. Fungsi pertama mengonversi kolom numerik menjadi Z-Score. Fungsi kedua mengganti nilai kategorikal dengan variabel dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "523a040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def encode_numeric_zscore(df, name):\n",
    "    \"\"\"\n",
    "    Apply z-score normalization to a specified numeric column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame containing the column.\n",
    "    name (str): The name of the column to normalize.\n",
    "    \"\"\"\n",
    "    mean = df[name].mean()\n",
    "    sd = df[name].std()\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "def encode_text_dummy(df, name):\n",
    "    \"\"\"\n",
    "    Convert a categorical column to dummy variables.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame containing the column.\n",
    "    name (str): The name of the categorical column.\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(df[name], prefix=name, dtype=float)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Process a DataFrame by encoding its features.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to process.\n",
    "    \"\"\"\n",
    "    for name in df.columns:\n",
    "        if name == 'outcome':\n",
    "            continue\n",
    "        #elif df[name].dtype == bool:\n",
    "        #    print(\"**\", name)\n",
    "        #    df[name] = df[name].astype(float)\n",
    "        elif name in ['protocol_type', 'service', 'flag', 'land', 'logged_in',\n",
    "                      'is_host_login', 'is_guest_login']:\n",
    "            df = encode_text_dummy(df, name)\n",
    "        else:\n",
    "            encode_numeric_zscore(df, name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fc360",
   "metadata": {},
   "source": [
    "Kode ini mengonversi semua kolom numerik menjadi Z-Score dan semua kolom teks menjadi variabel dummy. Selanjutnya, kita menggunakan fungsi-fungsi tersebut untuk melakukan prapemrosesan pada setiap kolom. Setelah program selesai memproses data, hasilnya kemudian ditampilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c05baeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration  src_bytes  dst_bytes  ...  is_host_login_0  is_guest_login_0  \\\n",
      "0 -0.067792  -0.002879   0.138664  ...              1.0               1.0   \n",
      "1 -0.067792  -0.002820  -0.011578  ...              1.0               1.0   \n",
      "2 -0.067792  -0.002824   0.014179  ...              1.0               1.0   \n",
      "3 -0.067792  -0.002840   0.014179  ...              1.0               1.0   \n",
      "4 -0.067792  -0.002842   0.035214  ...              1.0               1.0   \n",
      "\n",
      "   is_guest_login_1  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "\n",
      "[5 rows x 121 columns]\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 6)\n",
    "pd.set_option('display.max_rows', 5)\n",
    "\n",
    "df = process_dataframe(df)\n",
    "df.dropna(inplace=True, axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d700bb5",
   "metadata": {},
   "source": [
    "Kita membagi data menjadi dua kelompok, yaitu “normal” dan berbagai jenis serangan, untuk melakukan anomaly detection. Kode berikut membagi data tersebut ke dalam dua data frame dan menampilkan ukuran (jumlah data) dari masing-masing kelompok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e985ee09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count: 97278\n",
      "Attack count: 396743\n"
     ]
    }
   ],
   "source": [
    "normal_mask = df['outcome']=='normal.'\n",
    "attack_mask = df['outcome']!='normal.'\n",
    "\n",
    "df.drop('outcome',axis=1,inplace=True)\n",
    "\n",
    "df_normal = df[normal_mask]\n",
    "df_attack = df[attack_mask]\n",
    "\n",
    "print(f\"Normal count: {len(df_normal)}\")\n",
    "print(f\"Attack count: {len(df_attack)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf75486",
   "metadata": {},
   "source": [
    "Selanjutnya, kita mengonversi kedua data frame tersebut menjadi array Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a1336b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ini adalah vektor fitur numerik, karena akan dimasukkan ke dalam jaringan saraf (neural net)\n",
    "x_normal = df_normal.values\n",
    "x_attack = df_attack.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f0feb",
   "metadata": {},
   "source": [
    "## Training Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4825ea",
   "metadata": {},
   "source": [
    "Penting untuk dicatat bahwa kita tidak menggunakan kolom outcome sebagai label yang akan diprediksi. Kita akan melatih autoencoder menggunakan data “normal” dan kemudian melihat seberapa baik model tersebut dapat mendeteksi bahwa data yang tidak diberi label “normal” merupakan anomali. Deteksi anomali ini bersifat unsupervised, artinya tidak ada nilai target (y) yang diprediksi.\n",
    "\n",
    "Selanjutnya, kita membagi data normal menjadi 25% sebagai test set dan 75% sebagai train set. Program akan menggunakan data uji (test data) untuk memfasilitasi early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e8200149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_normal_train, x_normal_test = train_test_split(x_normal, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45e50d",
   "metadata": {},
   "source": [
    "Selanjutnya menampilkan ukuran (size) dari train set dan test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a48473",
   "metadata": {},
   "source": [
    "print(f\"Normal train count: {len(x_normal_train)}\")\n",
    "print(f\"Normal test count: {len(x_normal_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efd30c",
   "metadata": {},
   "source": [
    "Sekarang kita siap untuk melatih autoencoder menggunakan data normal. Autoencoder akan belajar untuk memampatkan (compress) data menjadi sebuah vektor yang hanya terdiri dari tiga angka. Autoencoder juga seharusnya mampu melakukan dekompresi (decompress) kembali dengan tingkat akurasi yang wajar. Seperti halnya pada autoencoder pada umumnya, kita melatih jaringan saraf agar menghasilkan nilai keluaran (output) yang sama dengan nilai yang dimasukkan ke lapisan input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9035fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/10], Loss: 0.32353480313577804\n",
      "Epoch [2/10], Loss: 0.2740984486387389\n",
      "Epoch [3/10], Loss: 0.24666339941649584\n",
      "Epoch [4/10], Loss: 0.2100492564005483\n",
      "Epoch [5/10], Loss: 0.20960935956283816\n",
      "Epoch [6/10], Loss: 0.19010148517207423\n",
      "Epoch [7/10], Loss: 0.20308898677620546\n",
      "Epoch [8/10], Loss: 0.18335483081397813\n",
      "Epoch [9/10], Loss: 0.18626280895499675\n",
      "Epoch [10/10], Loss: 0.16594917279782526\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors and move them to the appropriate device\n",
    "x_normal_train_tensor = torch.tensor(x_normal_train).float().to(device)\n",
    "x_normal_tensor = torch.tensor(x_normal).float().to(device)\n",
    "x_attack_tensor = torch.tensor(x_attack).float().to(device)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_data = TensorDataset(x_normal_train_tensor, x_normal_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model using Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_normal.shape[1], 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, x_normal.shape[1])\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    den = 0\n",
    "    for data in train_loader:\n",
    "        inputs, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss +=loss.item()\n",
    "        den+=1\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/den}')\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0786a3",
   "metadata": {},
   "source": [
    "# Mendeteksi Anomali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee47f3",
   "metadata": {},
   "source": [
    "Sekarang kita siap untuk melihat apakah data abnormal benar-benar merupakan anomali. Dua skor pertama menunjukkan kesalahan RMSE in-sample dan out-of-sample. Kedua skor ini relatif rendah, sekitar 0,33, karena berasal dari data normal. Sebaliknya, kesalahan yang jauh lebih tinggi, yaitu sekitar 0,76, muncul dari data abnormal. Autoencoder tidak mampu mengodekan data yang merepresentasikan serangan dengan baik. Kesalahan yang lebih tinggi ini menunjukkan adanya anomali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f0f5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor Normal Out of Sample (di luar sampel) (RMSE): 0.3557773530483246\n",
      "Skor Normal In-sample (dalam sampel) (RMSE): 0.37856152653694153\n",
      "Skor Saat Serangan Berlangsung (RMSE): 0.5233561396598816\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def calculate_rmse(model, data):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)\n",
    "        mse_loss = nn.MSELoss()(predictions, data)\n",
    "    return torch.sqrt(mse_loss).item()\n",
    "\n",
    "# Evaluating the model\n",
    "score1 = calculate_rmse(model, torch.tensor(x_normal_test).float().to(device))\n",
    "score2 = calculate_rmse(model, x_normal_tensor)\n",
    "score3 = calculate_rmse(model, x_attack_tensor)\n",
    "\n",
    "print(f\"Skor Normal Out of Sample (di luar sampel) (RMSE): {score1}\")\n",
    "print(f\"Skor Normal In-sample (dalam sampel) (RMSE): {score2}\")\n",
    "print(f\"Skor Saat Serangan Berlangsung (RMSE): {score3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd8225",
   "metadata": {},
   "source": [
    "# Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3c0cb",
   "metadata": {},
   "source": [
    "Untuntuk menguji model, kami menggunakan pertama data normal x_normal_test, x_normal_tensor, dan x_attack_tensor. Kami menghitung skor RMSE untuk ketiga data ini dan membandingkannya dengan skor RMSE yang didapat sebelumnya. Hasilnya adalah:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fd1af",
   "metadata": {},
   "source": [
    "Test score RMSE untuk data normal x_normal_test pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93462d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor data pertama Normal Out of Sample (di luar sampel) (RMSE): 0.0649721696972847\n"
     ]
    }
   ],
   "source": [
    "test_score_rmse_normal = calculate_rmse(model,torch.tensor(x_normal_test[0]).float().to(device))\n",
    "print(f\"Skor data pertama Normal Out of Sample (di luar sampel) (RMSE): {test_score_rmse_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8cac3",
   "metadata": {},
   "source": [
    "Test score RMSE untuk data normal sampel x_normal_tensor pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5431a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor data pertama Normal In-sample (dalam sampel) (RMSE): 0.1911206990480423\n"
     ]
    }
   ],
   "source": [
    "test_score_rmse_sample_normal = calculate_rmse(model, x_normal_tensor[0])\n",
    "print(f\"Skor data pertama Normal In-sample (dalam sampel) (RMSE): {test_score_rmse_sample_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f2925",
   "metadata": {},
   "source": [
    "Test score RMSE untuk data pertama dari x_attack_tensor saat serangan terjadi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5cba4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skor data pertama saat Serangan Terjadi (RMSE): 8.7673978805542\n"
     ]
    }
   ],
   "source": [
    "test_score_rmse_attack = calculate_rmse(model,x_attack_tensor[0])\n",
    "print(f\"Skor data pertama saat Serangan Terjadi (RMSE): {test_score_rmse_attack}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16adea2b",
   "metadata": {},
   "source": [
    "Dari ketiga Score di atas terlihat bahwa Score RMSE untuk Data Normal akan menghasilkan Score yang rendah dibandingkan dengan Score RMSE untuk Data yang terkena Serangan. Hal ini karena Model yang dibangun sudah terlatih dengan Data Normal dan Data yang terkena Serangan. Oleh karena itu, Model yang dibangun sudah dapat mengenali Data yang terkena Serangan dan Data Normal. Hal ini dapat dilihat dari hasil Score RMSE yang cukup rendah untuk Data Normal dan cukup tinggi untuk Data yang terkena Serangan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8128391",
   "metadata": {},
   "source": [
    "### Tentang Penulis\n",
    "\n",
    "Kajian ini ditulis oleh :\n",
    "- Nama : Doni Fristiyanto\n",
    "- Nim : 241012000122 \n",
    "- Kelas : v.340 (03MKME002)\n",
    "- Jurusan : Magister Teknik Informatika\n",
    "- Universitas : Universitas Pamulang\n",
    "- Tanggal :  1 November 2025\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
